# Comprehensive Analysis Outputs Guide

This document describes all the outputs generated by `analyze_parallel_results.py`.

## Overview

When you run:
```bash
python analyze_parallel_results.py --output-dir parallel_experiments
```

The analysis script generates **13 visualizations** and **5 CSV files** to help you understand:
- Which hyperparameters matter most
- Optimal configurations for each model
- Performance tradeoffs
- Model comparisons across datasets

## Generated Files

### ðŸ“Š CSV Files (5 files)

#### 1. `summary_statistics.csv`
**Purpose**: Statistical summary by model and dataset

**Contents**:
- Mean, std, min, max for RUL MAE
- Mean, std, min, max for RUL RMSE
- Mean, std, min, max for Concordance Index
- Mean, std for training duration

**Format**:
```csv
dataset,model_variant,rul_mae_mean,rul_mae_std,rul_mae_min,rul_mae_max,...
turbofan,lstm_basic,8.45,0.82,7.21,10.12,...
turbofan,transformer_basic,7.82,0.65,6.89,9.34,...
azure_pm,lstm_basic,12.34,1.21,10.45,14.56,...
```

**Use Case**: Quick overview of model performance ranges

---

#### 2. `best_configurations.csv`
**Purpose**: Best hyperparameters for each model variant

**Contents**:
- Best config by MAE (lowest error)
- Best config by C-Index (highest survival ranking)

**Format**:
```csv
model,criterion,exp_name,batch_size,learning_rate,lambda_param,nasa_weight,dropout,rul_mae,rul_rmse,concordance_index
lstm_basic,MAE,turbofan/lstm_basic/bs128_lr0.001...,128,0.001,0.75,0.1,0.1,7.21,10.45,0.92
lstm_basic,C-Index,turbofan/lstm_basic/bs256_lr0.001...,256,0.001,0.75,0.0,0.15,7.45,10.67,0.94
```

**Use Case**: Find optimal hyperparameters to retrain your best model

---

#### 3. `top_20_by_mae.csv`
**Purpose**: Top 20 configurations ranked by lowest MAE (best error)

**Contents**: All hyperparameters and metrics for top 20 experiments

**Use Case**:
- Identify best overall configurations
- Look for patterns in top performers
- Compare different models at their best

---

#### 4. `top_20_by_cindex.csv`
**Purpose**: Top 20 configurations ranked by highest C-Index (best survival ranking)

**Use Case**:
- Find models best at ranking survival times
- Useful when ranking quality matters more than absolute error

---

#### 5. `top_20_by_nasa_score.csv`
**Purpose**: Top 20 configurations by NASA/PHM08 scoring function

**Use Case**:
- Optimize for asymmetric cost (late predictions penalized more)
- Competition scoring
- Real-world predictive maintenance where late failures are critical

---

### ðŸ“ˆ Visualizations (13 plots in `analysis_plots/`)

#### 1. `rul_mae_vs_hyperparameters.png`
**Type**: 5-panel error bar plot
**Shows**: Effect of each hyperparameter on RUL MAE

**Panels**:
- Batch size vs MAE
- Learning rate vs MAE
- Lambda parameter vs MAE
- NASA weight vs MAE
- Dropout vs MAE

**How to Read**:
- Lower values = better
- Error bars show variability
- Look for clear trends or plateaus
- `n=X` annotations show sample size

**Use Case**:
- Identify which hyperparameters most affect error
- Find optimal values or ranges

**Example Insights**:
- "MAE lowest at batch_size=128, increases above 256"
- "Learning rate 0.001 consistently best across models"

---

#### 2. `rul_rmse_vs_hyperparameters.png`
**Type**: 5-panel error bar plot
**Shows**: Same as above but for RMSE (penalizes large errors more)

**Use Case**:
- When you care more about avoiding large errors
- RMSE is more sensitive to outliers than MAE

---

#### 3. `concordance_index_vs_hyperparameters.png`
**Type**: 5-panel error bar plot
**Shows**: Effect on survival ranking quality

**How to Read**:
- Higher values = better (max is 1.0)
- C-Index = 0.5 is random guessing
- C-Index > 0.7 is good, > 0.8 is very good

**Use Case**:
- Optimize for ranking quality
- Important for survival analysis applications

---

#### 4. `model_comparison.png`
**Type**: 3-panel boxplot
**Shows**: Distribution of performance across all configurations for each model

**Panels**:
- MAE distribution by model
- RMSE distribution by model
- C-Index distribution by model

**How to Read**:
- Box shows 25th-75th percentile
- Line in box is median
- Whiskers show min/max (excluding outliers)
- Dots are outliers

**Use Case**:
- Compare model robustness (narrow boxes = consistent)
- Identify best performing model families
- Spot models with high variance

**Example Insights**:
- "Transformers have lower median MAE but higher variance"
- "LSTM models more consistent across hyperparameters"

---

#### 5. `batch_size_analysis.png`
**Type**: 4-panel line plot with error bars
**Shows**: Comprehensive batch size impact

**Panels**:
- **Top-left**: Batch size vs MAE (accuracy)
- **Top-right**: Batch size vs RMSE (accuracy)
- **Bottom-left**: Batch size vs C-Index (ranking)
- **Bottom-right**: Batch size vs Training Time (efficiency)

**How to Read**:
- Each line is a different model variant
- Error bars show std across other hyperparameters
- Look for sweet spots balancing performance and speed

**Use Case**:
- Find optimal batch size for your GPU memory
- Understand speed vs accuracy tradeoff

**Example Insights**:
- "Batch 128 has good performance and 2Ã— faster than batch 32"
- "Performance plateaus above batch 256 for transformers"
- "LSTMs less sensitive to batch size than transformers"

---

#### 6. `learning_rate_analysis.png`
**Type**: 3-panel line plot (log scale x-axis)
**Shows**: Learning rate sensitivity

**Panels**:
- LR vs MAE
- LR vs RMSE
- LR vs C-Index

**How to Read**:
- X-axis is logarithmic (0.0001, 0.001, 0.01, etc.)
- Look for clear optimal ranges
- Too low LR = slow convergence = underfitting
- Too high LR = instability = bad performance

**Use Case**:
- Identify optimal learning rate range
- Understand model sensitivity to LR

**Example Insights**:
- "Transformers work best at LR=0.001"
- "LSTMs more robust to LR variation"
- "Performance drops sharply above LR=0.01"

---

#### 7. `lambda_parameter_analysis.png`
**Type**: 4-panel line plot
**Shows**: Effect of lambda (DDRSA loss weight) on performance

**Panels**:
- Lambda vs MAE
- Lambda vs RMSE
- Lambda vs C-Index
- Lambda vs Training Time

**How to Read**:
- Lambda balances survival prediction vs hazard prediction
- Lambda = 0.5: equal weight
- Lambda = 0.75: 3Ã— more weight on survival
- Lambda = 1.0: only survival loss

**Use Case**:
- Optimize DDRSA-specific hyperparameter
- Understand tradeoff between loss components

**Example Insights**:
- "Lambda=0.75 best for most models (paper default)"
- "Higher lambda improves C-Index but increases MAE"

---

#### 8. `dropout_analysis.png`
**Type**: 4-panel line plot
**Shows**: Dropout rate impact on overfitting and performance

**Panels**:
- Dropout vs MAE
- Dropout vs RMSE
- Dropout vs C-Index
- Dropout vs Training Time

**How to Read**:
- Dropout = 0.0: no regularization (may overfit)
- Dropout = 0.5: strong regularization (may underfit)
- Look for U-shaped curves (too little or too much is bad)

**Use Case**:
- Find optimal regularization strength
- Prevent overfitting on transformers

**Example Insights**:
- "Transformers benefit from dropout=0.1-0.2"
- "LSTMs work well even with dropout=0"
- "Training time slightly increases with dropout"

---

#### 9. `nasa_loss_impact.png`
**Type**: 4-panel line plot
**Shows**: Effect of NASA loss weight on metrics

**Panels**:
- NASA weight vs MAE
- NASA weight vs RMSE
- NASA weight vs C-Index
- NASA weight vs NASA Score

**How to Read**:
- NASA weight = 0.0: NASA loss disabled
- NASA weight = 0.1-0.5: NASA loss included
- Higher weight â†’ optimize more for NASA scoring
- May increase MAE but decrease NASA score

**Use Case**:
- Optimize for PHM08 competition scoring
- Balance general accuracy vs NASA-specific metric

**Example Insights**:
- "NASA weight=0.1 reduces NASA score without hurting MAE much"
- "Weight above 0.2 starts trading off accuracy for scoring"

---

#### 10. `dataset_comparison.png`
**Type**: 3-panel grouped bar chart
**Shows**: Model performance on different datasets (turbofan vs azure_pm)

**Panels**:
- MAE by dataset
- RMSE by dataset
- C-Index by dataset

**How to Read**:
- Side-by-side bars for each model variant
- Different colors for different datasets
- Error bars show variability

**Use Case**:
- Understand dataset difficulty
- Identify models that generalize well
- Compare dataset characteristics

**Example Insights**:
- "Azure PM harder than Turbofan (higher MAE)"
- "Transformers perform better on Azure PM"
- "Some models show dataset-specific strengths"

---

#### 11. `training_efficiency.png`
**Type**: 3-panel scatter plot
**Shows**: Training time vs performance tradeoff

**Panels**:
- Training time vs MAE
- Training time vs RMSE
- Training time vs C-Index

**How to Read**:
- Each point is one experiment
- Colors/markers distinguish models
- Bottom-left corner = fast and accurate (best)
- Top-right corner = slow and inaccurate (worst)

**Use Case**:
- Find efficient models (good performance, low time)
- Identify diminishing returns
- Budget time vs accuracy tradeoff

**Example Insights**:
- "LSTMs train 3Ã— faster than transformers with similar accuracy"
- "ProbSparse models slow but worth it for azure_pm dataset"
- "Some transformer configs train long but underperform"

---

#### 12. `hyperparameter_heatmap.png`
**Type**: 2-panel heatmap
**Shows**: Interaction between batch size and learning rate

**Panels**:
- **Left**: MAE heatmap (lower = greener)
- **Right**: C-Index heatmap (higher = greener)

**How to Read**:
- Rows = batch sizes
- Columns = learning rates
- Color intensity = performance
- Numbers show exact values

**Use Case**:
- Identify optimal batch size Ã— LR combinations
- See if effects are independent or interactive

**Example Insights**:
- "Best performance at (batch=128, LR=0.001)"
- "High batch size needs lower learning rate"
- "Some combinations consistently bad (avoid)"

---

#### 13. `lambda_nasa_heatmap.png`
**Type**: 2-panel heatmap
**Shows**: Interaction between lambda and NASA weight

**Use Case**:
- Optimize DDRSA-specific hyperparameters together
- Understand if they interact or are independent

---

#### 14. `dropout_lr_heatmap.png`
**Type**: 2-panel heatmap
**Shows**: Interaction between dropout and learning rate

**Use Case**:
- Find optimal regularization Ã— optimization combination
- Higher LR may need more dropout to stabilize

---

## Complete Analysis Workflow

### Step 1: Run Experiments
```bash
python run_parallel_experiments.py \
  --batch-sizes 64 128 256 \
  --learning-rates 0.0005 0.001 0.005 \
  --lambda-params 0.5 0.6 0.75 \
  --nasa-weights 0.0 0.05 0.1 \
  --dropout-rates 0.1 0.15 0.2 \
  --output-dir comprehensive_search
```

### Step 2: Analyze Results
```bash
python analyze_parallel_results.py --output-dir comprehensive_search
```

### Step 3: Review Outputs

**Quick Assessment**:
1. Check `top_20_by_mae.csv` - What are the best configs overall?
2. Look at `model_comparison.png` - Which model family is best?
3. Check `batch_size_analysis.png` - What batch size balances speed and accuracy?

**Deep Dive**:
4. Review `learning_rate_analysis.png` - Is my LR range optimal?
5. Check `lambda_parameter_analysis.png` - Is lambda=0.75 still best?
6. Look at `nasa_loss_impact.png` - Should I use NASA loss?
7. Review `dropout_analysis.png` - Am I overfitting?

**Hyperparameter Interactions**:
8. Check `hyperparameter_heatmap.png` - Any unexpected combinations?
9. Look at `lambda_nasa_heatmap.png` - How do DDRSA params interact?
10. Review `dropout_lr_heatmap.png` - Regularization vs optimization balance?

**Dataset Insights**:
11. Check `dataset_comparison.png` - How do models generalize?
12. Look at `training_efficiency.png` - What's the time vs accuracy tradeoff?

### Step 4: Select Best Configuration

From `best_configurations.csv`, choose your model. For example:
```csv
transformer_basic,MAE,...,128,0.001,0.75,0.1,0.15,7.82,11.54,0.92
```

### Step 5: Retrain Best Model
```bash
python main.py \
  --dataset turbofan \
  --model-type transformer \
  --batch-size 128 \
  --learning-rate 0.001 \
  --lambda-param 0.75 \
  --nasa-weight 0.1 \
  --dropout 0.15 \
  --num-epochs 300 \
  --create-visualization \
  --exp-name final_best_model
```

## Interpreting Results: Common Patterns

### Pattern 1: Batch Size Sweet Spot
**Observation**: Performance plateaus around batch_size=128-256

**Interpretation**:
- Below 128: Noisy gradients, slower convergence
- 128-256: Optimal balance
- Above 256: Diminishing returns, less regularization effect

**Action**: Use batch_size=128 or 256

---

### Pattern 2: Learning Rate Cliff
**Observation**: Performance good at LR=0.001, drops sharply at LR=0.01

**Interpretation**:
- 0.0001: Too slow, underfitting
- 0.001: Sweet spot
- 0.01: Too fast, unstable

**Action**: Use LR around 0.001, avoid extremes

---

### Pattern 3: Lambda Tradeoff
**Observation**: Higher lambda â†’ better C-Index, worse MAE

**Interpretation**:
- Lambda weights survival vs hazard loss
- Higher lambda optimizes for ranking
- Lower lambda optimizes for point estimates

**Action**: Use lambda=0.75 for balance, or tune based on your metric priority

---

### Pattern 4: NASA Loss Double-Edged Sword
**Observation**: NASA weight=0.1 reduces NASA score but may increase MAE

**Interpretation**:
- NASA loss penalizes late predictions asymmetrically
- Helps with competition scoring
- May hurt general accuracy

**Action**: Use NASA weight=0.1 if entering PHM08-style competition, else 0.0

---

### Pattern 5: Model-Specific Sensitivities
**Observation**: Transformers more sensitive to hyperparameters than LSTMs

**Interpretation**:
- Transformers have more hyperparameters to tune
- LSTMs more robust but lower ceiling
- Transformers need dropout, LSTMs work without

**Action**:
- LSTMs: Start with defaults, minimal tuning
- Transformers: Invest time in hyperparameter search

---

## Quick Reference Table

| Want to Know | Check This File | Look For |
|--------------|----------------|----------|
| **Best overall config** | `top_20_by_mae.csv` | Lowest MAE in top row |
| **Best model family** | `model_comparison.png` | Lowest median box |
| **Optimal batch size** | `batch_size_analysis.png` | Lowest point on curves |
| **Optimal learning rate** | `learning_rate_analysis.png` | Minimum on curve |
| **Is lambda=0.75 best?** | `lambda_parameter_analysis.png` | Compare curves |
| **Should I use dropout?** | `dropout_analysis.png` | Compare dropout=0 vs 0.1-0.2 |
| **Use NASA loss?** | `nasa_loss_impact.png` | Check if nasa_weight>0 helps |
| **Which dataset harder?** | `dataset_comparison.png` | Higher bars |
| **Fast vs slow models** | `training_efficiency.png` | Bottom-left is best |
| **Hyperparameter interactions** | `*_heatmap.png` | Green regions |

## Summary

The analysis provides:
- âœ… **5 CSV files** with rankings and configurations
- âœ… **13 visualization types** covering all aspects
- âœ… Statistical summaries and best configs
- âœ… Hyperparameter sensitivity analysis
- âœ… Model comparisons across datasets
- âœ… Training efficiency analysis

**Use this to**:
1. Understand which hyperparameters matter most
2. Find optimal configurations for your models
3. Make informed tradeoffs (speed vs accuracy)
4. Identify robust vs fragile model architectures
5. Select the best model for production deployment
